{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2565c47-8e81-4c10-b252-5dbeb7319e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import collections\n",
    "import time\n",
    "import pika\n",
    "\n",
    "from IPython import display\n",
    "from typing import Tuple, Dict\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import ops\n",
    "from ultralytics.utils.plotting import colors\n",
    "from notebook_utils import download_file, VideoPlayer\n",
    "from openvino.runtime import Core, Model\n",
    "\n",
    "core = Core()\n",
    "core.available_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30161bdb-425d-4ddc-a790-7b99279adb66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rtsp_src = os.environ.get(\n",
    "    'RTSP_SRC', 'rtsp://192.168.0.21:8554/unicast'\n",
    ")\n",
    "print(rtsp_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ad8a0-a1c4-4018-a5c4-c9188868ed04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f751411-2d99-42d2-9753-c58c3cc669a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_dir = Path('.')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "device = os.environ.get(\n",
    "    'ACCELERATION_DEVICE', 'CPU'\n",
    ")\n",
    "\n",
    "DET_MODEL_NAME = \"yolov8l\"\n",
    "#SEG_MODEL_NAME = \"yolov8n-seg\"\n",
    "\n",
    "det_model = YOLO(f\"{DET_MODEL_NAME}.yaml\")\n",
    "det_model = YOLO(f\"{DET_MODEL_NAME}.pt\")\n",
    "\n",
    "#seg_model = YOLO(f\"{SEG_MODEL_NAME}.yaml\")\n",
    "#seg_model = YOLO(f\"{SEG_MODEL_NAME}.pt\")\n",
    "\n",
    "# object detection model\n",
    "det_model_path = models_dir / f\"{DET_MODEL_NAME}_openvino_model/{DET_MODEL_NAME}.xml\"\n",
    "if not det_model_path.exists():\n",
    "    det_model.export(format=\"openvino\", dynamic=True, half=False)\n",
    "\n",
    "# instance segmentation model\n",
    "#seg_model_path = models_dir / f\"{SEG_MODEL_NAME}_openvino_model/{SEG_MODEL_NAME}.xml\"\n",
    "#if not seg_model_path.exists():\n",
    "#    seg_model.export(format=\"openvino\", dynamic=True, half=False)\n",
    "\n",
    "det_ov_model = core.read_model(det_model_path)\n",
    "#seg_ov_model = core.read_model(seg_model_path)\n",
    "\n",
    "if device != \"CPU\":\n",
    "    det_ov_model.reshape({0: [1, 3, 640, 640]})\n",
    "    available_devices = core.available_devices\n",
    "    print(available_devices)\n",
    "\n",
    "print(device)\n",
    "\n",
    "label_map = det_model.model.names\n",
    "print(label_map)\n",
    "\n",
    "print(type(det_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8307e3-196b-4bc7-91f5-065a4acef888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "det_compiled_model = core.compile_model(det_ov_model, device)\n",
    "#seg_compiled_model = core.compile_model(seg_ov_model, device)\n",
    "\n",
    "if not det_model_path.exists():\n",
    "    det_model.export(format=\"openvino\", dynamic=True, half=False)\n",
    "#if not seg_model_path.exists():\n",
    "    seg_model.export(format=\"openvino\", dynamic=True, half=False)\n",
    "\n",
    "print(type(det_compiled_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d06672-f2d3-458c-8a81-0d12aee4aec4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### plot_one_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac29e7a-c70c-4e82-94d9-f2dabbef34da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_one_box(box:np.ndarray, img:np.ndarray, color:Tuple[int, int, int] = None, mask:np.ndarray = 1, label:str = None, line_thickness:int = 5):\n",
    "    \"\"\"\n",
    "    Helper function for drawing single bounding box on image\n",
    "    Parameters:\n",
    "        x (np.ndarray): bounding box coordinates in format [x1, y1, x2, y2]\n",
    "        img (no.ndarray): input image\n",
    "        color (Tuple[int, int, int], *optional*, None): color in BGR format for drawing box, if not specified will be selected randomly\n",
    "        mask (np.ndarray, *optional*, None): instance segmentation mask polygon in format [N, 2], where N - number of points in contour, if not provided, only box will be drawn\n",
    "        label (str, *optonal*, None): box label string, if not provided will not be provided as drowing result\n",
    "        line_thickness (int, *optional*, 5): thickness for box drawing lines\n",
    "    \"\"\"\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "    if mask is not None:\n",
    "        image_with_mask = img.copy()\n",
    "        mask\n",
    "        cv2.fillPoly(image_with_mask, pts=[mask.astype(int)], color=color)\n",
    "        img = cv2.addWeighted(img, 0.5, image_with_mask, 0.5, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a771a41-411a-4062-a873-26931d1b0bcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### draw_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fdda2-214b-4ee4-8bec-b0d71b349ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_results(results:Dict, source_image:np.ndarray, label_map:Dict):\n",
    "    \"\"\"\n",
    "    Helper function for drawing bounding boxes on image\n",
    "    Parameters:\n",
    "        image_res (np.ndarray): detection predictions in format [x1, y1, x2, y2, score, label_id]\n",
    "        source_image (np.ndarray): input image for drawing\n",
    "        label_map; (Dict[int, str]): label_id to class name mapping\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    boxes = results[\"det\"]\n",
    "    masks = results.get(\"segment\")\n",
    "    h, w = source_image.shape[:2]\n",
    "    \n",
    "    for idx, (*xyxy, conf, lbl) in enumerate(boxes):\n",
    "        #label = f'{label_map[int(lbl)]}'\n",
    "        label = f'{label_map[int(lbl)]} {conf:.2f}'\n",
    "        #label = \"masanara\"\n",
    "        mask = masks[idx] if masks is not None else None\n",
    "        source_image = plot_one_box(xyxy, source_image, mask=mask, label=label, color=colors(int(lbl)), line_thickness=1)\n",
    "    return source_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfb6c4-d681-42d5-bf31-d86299a43261",
   "metadata": {},
   "source": [
    "### letterbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c874f3-d235-43f6-be63-c5c904c55a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def letterbox(img: np.ndarray, new_shape:Tuple[int, int] = (640, 640), color:Tuple[int, int, int] = (114, 114, 114), auto:bool = False, scale_fill:bool = False, scaleup:bool = False, stride:int = 32):\n",
    "    \"\"\"\n",
    "    Resize image and padding for detection. Takes image as input, \n",
    "    resizes image to fit into new shape with saving original aspect ratio and pads it to meet stride-multiple constraints\n",
    "    \n",
    "    Parameters:\n",
    "      img (np.ndarray): image for preprocessing\n",
    "      new_shape (Tuple(int, int)): image size after preprocessing in format [height, width]\n",
    "      color (Tuple(int, int, int)): color for filling padded area\n",
    "      auto (bool): use dynamic input size, only padding for stride constrins applied\n",
    "      scale_fill (bool): scale image to fill new_shape\n",
    "      scaleup (bool): allow scale image if it is lower then desired input size, can affect model accuracy\n",
    "      stride (int): input padding stride\n",
    "    Returns:\n",
    "      img (np.ndarray): image after preprocessing\n",
    "      ratio (Tuple(float, float)): hight and width scaling ratio\n",
    "      padding_size (Tuple(int, int)): height and width padding size\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scale_fill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ee1c6-1b9c-4c9a-9ee8-5c6d1b5c00b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### preprocess_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077b3b7-e763-4d09-abfc-0aca6e690a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img0: np.ndarray):\n",
    "    \"\"\"\n",
    "    Preprocess image according to YOLOv8 input requirements. \n",
    "    Takes image in np.array format, resizes it to specific size using letterbox resize and changes data layout from HWC to CHW.\n",
    "    \n",
    "    Parameters:\n",
    "      img0 (np.ndarray): image for preprocessing\n",
    "    Returns:\n",
    "      img (np.ndarray): image after preprocessing\n",
    "    \"\"\"\n",
    "    # resize\n",
    "    img = letterbox(img0)[0]\n",
    "    \n",
    "    # Convert HWC to CHW\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = np.ascontiguousarray(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd2168-f45b-421f-902f-6f29298d2a03",
   "metadata": {},
   "source": [
    "### image_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df3427-1800-456a-9624-80252b90dd7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_to_tensor(image:np.ndarray):\n",
    "    \"\"\"\n",
    "    Preprocess image according to YOLOv8 input requirements. \n",
    "    Takes image in np.array format, resizes it to specific size using letterbox resize and changes data layout from HWC to CHW.\n",
    "    \n",
    "    Parameters:\n",
    "      img (np.ndarray): image for preprocessing\n",
    "    Returns:\n",
    "      input_tensor (np.ndarray): input tensor in NCHW format with float32 values in [0, 1] range \n",
    "    \"\"\"\n",
    "    input_tensor = image.astype(np.float32)  # uint8 to fp32\n",
    "    input_tensor /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    \n",
    "    # add batch dimension\n",
    "    if input_tensor.ndim == 3:\n",
    "        input_tensor = np.expand_dims(input_tensor, 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d04dd4-c281-4016-881a-90a31a45f2c6",
   "metadata": {},
   "source": [
    "### postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15ee93-cb3d-4627-adc4-db62b83676e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess(\n",
    "    pred_boxes:np.ndarray,\n",
    "    input_hw:Tuple[int, int],\n",
    "    orig_img:np.ndarray,\n",
    "    min_conf_threshold:float = 0.25,\n",
    "    nms_iou_threshold:float = 0.7,\n",
    "    agnosting_nms:bool = False,\n",
    "    max_detections:int = 300,\n",
    "    pred_masks:np.ndarray = None,\n",
    "    retina_mask:bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    YOLOv8 model postprocessing function. Applied non maximum supression algorithm to detections and rescale boxes to original image size\n",
    "    Parameters:\n",
    "        pred_boxes (np.ndarray): model output prediction boxes\n",
    "        input_hw (np.ndarray): preprocessed image\n",
    "        orig_image (np.ndarray): image before preprocessing\n",
    "        min_conf_threshold (float, *optional*, 0.25): minimal accepted confidence for object filtering\n",
    "        nms_iou_threshold (float, *optional*, 0.45): minimal overlap score for removing objects duplicates in NMS\n",
    "        agnostic_nms (bool, *optiona*, False): apply class agnostinc NMS approach or not\n",
    "        max_detections (int, *optional*, 300):  maximum detections after NMS\n",
    "        pred_masks (np.ndarray, *optional*, None): model ooutput prediction masks, if not provided only boxes will be postprocessed\n",
    "        retina_mask (bool, *optional*, False): retina mask postprocessing instead of native decoding\n",
    "    Returns:\n",
    "       pred (List[Dict[str, np.ndarray]]): list of dictionary with det - detected boxes in format [x1, y1, x2, y2, score, label] and segment - segmentation polygons for each element in batch\n",
    "    \"\"\"\n",
    "    nms_kwargs = {\"agnostic\": agnosting_nms, \"max_det\":max_detections}\n",
    "    # if pred_masks is not None:\n",
    "    #     nms_kwargs[\"nm\"] = 32\n",
    "    preds = ops.non_max_suppression(\n",
    "        torch.from_numpy(pred_boxes),\n",
    "        min_conf_threshold,\n",
    "        nms_iou_threshold,\n",
    "        nc=80,\n",
    "        **nms_kwargs\n",
    "    )\n",
    "    results = []\n",
    "    proto = torch.from_numpy(pred_masks) if pred_masks is not None else None\n",
    "\n",
    "    for i, pred in enumerate(preds):\n",
    "        shape = orig_img[i].shape if isinstance(orig_img, list) else orig_img.shape\n",
    "        if not len(pred):\n",
    "            results.append({\"det\": [], \"segment\": []})\n",
    "            continue\n",
    "        if proto is None:\n",
    "            pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "            results.append({\"det\": pred})\n",
    "            continue\n",
    "        if retina_mask:\n",
    "            pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "            masks = ops.process_mask_native(proto[i], pred[:, 6:], pred[:, :4], shape[:2])  # HWC\n",
    "            segments = [ops.scale_segments(input_hw, x, shape, normalize=False) for x in ops.masks2segments(masks)]\n",
    "        else:\n",
    "            masks = ops.process_mask(proto[i], pred[:, 6:], pred[:, :4], input_hw, upsample=True)\n",
    "            pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "            segments = [ops.scale_segments(input_hw, x, shape, normalize=False) for x in ops.masks2segments(masks)]\n",
    "        results.append({\"det\": pred[:, :6].numpy(), \"segment\": segments})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f104b8-6034-49cc-8860-f52af0361269",
   "metadata": {
    "tags": []
   },
   "source": [
    "### detect\n",
    "- image_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db142f9-0e5b-495e-b53c-b8f600320d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect(image:np.ndarray, model:Model):\n",
    "    \"\"\"\n",
    "    OpenVINO YOLOv8 model inference function. Preprocess image, runs model inference and postprocess results using NMS.\n",
    "    Parameters:\n",
    "        image (np.ndarray): input image.\n",
    "        model (Model): OpenVINO compiled model.\n",
    "    Returns:\n",
    "        detections (np.ndarray): detected boxes in format [x1, y1, x2, y2, score, label]\n",
    "    \"\"\"\n",
    "    num_outputs = len(model.outputs)\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    input_tensor = image_to_tensor(preprocessed_image)\n",
    "    result = model(input_tensor)\n",
    "    boxes = result[model.output(0)]\n",
    "    masks = None\n",
    "    if num_outputs > 1:\n",
    "        masks = result[model.output(1)]\n",
    "    input_hw = input_tensor.shape[2:]\n",
    "    detections = postprocess(pred_boxes=boxes, input_hw=input_hw, orig_img=image, pred_masks=masks)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8b9b5-9568-491d-9005-4a32957bdb04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### sysout_results (out to STDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de6e8d-5eeb-4333-ad83-8b08a0dc5199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sysout_results(results:Dict, source_image:np.ndarray, label_map:Dict):\n",
    "    \"\"\"\n",
    "    Helper function for drawing bounding boxes on image\n",
    "    Parameters:\n",
    "        image_res (np.ndarray): detection predictions in format [x1, y1, x2, y2, score, label_id]\n",
    "        source_image (np.ndarray): input image for drawing\n",
    "        label_map; (Dict[int, str]): label_id to class name mapping\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    boxes = results[\"det\"]\n",
    "    masks = results.get(\"segment\")\n",
    "    h, w = source_image.shape[:2]\n",
    "    p=0\n",
    "    for idx, (*xyxy, conf, lbl) in enumerate(boxes):\n",
    "        label = f'{label_map[int(lbl)]} {conf:.2f}'\n",
    "        object_class = label_map[int(lbl)]\n",
    "        if object_class == \"person\":\n",
    "            p=p+1\n",
    "        \"\"\"\n",
    "        messageBody = '{\"class\": \"' + label_map[int(lbl)] \\\n",
    "               + '\", \"score\": \"' + str(conf.item()) \\\n",
    "               + '\", \"x1\": \"' + str(xyxy[0].item()) \\\n",
    "               + '\", \"y1\": \"' + str(xyxy[1].item()) + '\" }'\n",
    "        print(messageBody)\n",
    "        \"\"\"\n",
    "    messageBody = '{\"class\": \"person\", \"count\": \"' + str(p) + '\" }'\n",
    "    print(messageBody)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742dd902-1a19-4a96-be26-b3eeda3ccf95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### stream_results (out to RabbitMQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ee09e-8396-4412-bc82-6dabad9dc5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rabbitmq_hostname = os.environ.get(\n",
    "    'AMQP_HOSTNAME', 'localhost'\n",
    ")\n",
    "rabbitmq_port = os.environ.get(\n",
    "    'AMQP_PORT', '5672'\n",
    ")\n",
    "rabbitmq_user = os.environ.get(\n",
    "    'AMQP_USER', 'user'\n",
    ")\n",
    "rabbitmq_password = os.environ.get(\n",
    "    'AMQP_PASSWORD', 'guest'\n",
    ")\n",
    "\n",
    "print (rabbitmq_hostname)\n",
    "print (rabbitmq_user, rabbitmq_password)\n",
    "\n",
    "credentials = pika.PlainCredentials(rabbitmq_user, rabbitmq_password)\n",
    "parameters = pika.ConnectionParameters(rabbitmq_hostname,\n",
    "                                       rabbitmq_port,\n",
    "                                       '/',\n",
    "                                       credentials)\n",
    "\n",
    "connection = pika.BlockingConnection(parameters)\n",
    "channel = connection.channel() # start a channel\n",
    "\n",
    "# Delete a Stream, named test_stream\n",
    "#channel.queue_delete(queue='inferencing_stream')\n",
    "\n",
    "# Declare a Stream, named test_stream\n",
    "channel.queue_declare(\n",
    "  queue='inferencing_stream',\n",
    "      durable=True,\n",
    "  arguments={\"x-queue-type\": \"stream\", \"x-max-age\": \"1m\"}\n",
    ")\n",
    "\n",
    "#global prev_message\n",
    "#prev_message = '{\"class\": \"person\", \"count\": \"0\"}'\n",
    "\n",
    "def stream_results(results:Dict, source_image:np.ndarray, label_map:Dict, prev_message):\n",
    "    boxes = results[\"det\"]\n",
    "    masks = results.get(\"segment\")\n",
    "    h, w = source_image.shape[:2]\n",
    "    p = 0\n",
    "    \n",
    "    for idx, (*xyxy, conf, lbl) in enumerate(boxes):\n",
    "        label = f'{label_map[int(lbl)]} {conf:.2f}'\n",
    "        object_class = label_map[int(lbl)]\n",
    "        print(object_class)\n",
    " \n",
    "        if object_class == \"person\":\n",
    "            p=p+1\n",
    "    print(p)\n",
    "    \n",
    "    messageBody = '{\"class\": \"person\", \"count\": \"' + str(p) + '\"}'\n",
    "    \n",
    "    if not prev_message == messageBody:\n",
    "        \n",
    "        channel.basic_publish(\n",
    "            exchange='',\n",
    "            routing_key='inferencing_stream',\n",
    "            body=messageBody\n",
    "        )\n",
    "        \n",
    "        prev_message = messageBody\n",
    "    \n",
    "    print(messageBody)\n",
    "    return prev_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a640e-6140-429f-ac3c-5e83d25eb823",
   "metadata": {},
   "source": [
    "# Main processing function to run object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ec769-6611-42a1-8947-e8c5cffd7e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_object_detection(source=0, flip=False, use_popup=False, skip_first_frames=0, model=det_model, device=device):\n",
    "    player = None\n",
    "    \n",
    "    publishSysout = False\n",
    "    publishRMQ = True\n",
    "    publishDisplay = True\n",
    "    \n",
    "    mc = None\n",
    "    \n",
    "    if device != \"CPU\":\n",
    "        model.reshape({0: [1, 3, 640, 640]})\n",
    "        \n",
    "    compiled_model = core.compile_model(model, device)\n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = VideoPlayer(\n",
    "            source=source, flip=flip, fps=1, skip_first_frames=skip_first_frames, scale=1.0\n",
    "        )\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(\n",
    "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
    "            )\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        #prev_card_code = ''\n",
    "        prev_message = ''\n",
    "        \n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            # Get the results.\n",
    "            input_image = np.array(frame)\n",
    "           \n",
    "            start_time = time.time()\n",
    "            # model expects RGB image, while video capturing in BGR\n",
    "            detections = detect(input_image[:, :, ::-1], compiled_model)[0]\n",
    "            stop_time = time.time()\n",
    "            \n",
    "            if publishRMQ:\n",
    "                prev_message = stream_results(detections, input_image, label_map, prev_message)\n",
    "                \n",
    "            if publishSysout:\n",
    "                sysout_results(detections, input_image, label_map)\n",
    "            \n",
    "            if publishDisplay:\n",
    "                image_with_boxes = draw_results(detections, input_image, label_map)\n",
    "                frame = image_with_boxes\n",
    "\n",
    "                processing_times.append(stop_time - start_time)\n",
    "                # Use processing times from last 200 frames.\n",
    "                if len(processing_times) > 200:\n",
    "                    processing_times.popleft()\n",
    "\n",
    "                _, f_width = frame.shape[:2]\n",
    "                # Mean processing time [ms].\n",
    "                processing_time = np.mean(processing_times) * 1000\n",
    "                fps = 1000 / processing_time\n",
    "                cv2.putText(\n",
    "                    img=frame,\n",
    "                    text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                    org=(20, 40),\n",
    "                    fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    fontScale=f_width / 1000,\n",
    "                    color=(0, 0, 255),\n",
    "                    thickness=1,\n",
    "                    lineType=cv2.LINE_AA,\n",
    "                )\n",
    "                # Use this workaround if there is flickering.\n",
    "                if use_popup:\n",
    "                    cv2.imshow(winname=title, mat=frame)\n",
    "                    key = cv2.waitKey(1)\n",
    "                    # escape = 27\n",
    "                    if key == 27:\n",
    "                        break\n",
    "                else:\n",
    "                    # Encode numpy array to jpg.\n",
    "                    _, encoded_img = cv2.imencode(\n",
    "                        ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
    "                    )\n",
    "                    # Create an IPython image.\n",
    "                    i = display.Image(data=encoded_img)\n",
    "                    # Display the image in this notebook.\n",
    "                    display.clear_output(wait=True)\n",
    "                    display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874edb0-fe01-4f21-949b-05e4e5321a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    WEBCAM_INFERENCE = True\n",
    "\n",
    "    if WEBCAM_INFERENCE:\n",
    "        # VIDEO_SOURCE = 0  # Webcam\n",
    "        VIDEO_SOURCE = rtsp_src\n",
    "    else:\n",
    "        VIDEO_SOURCE = 'https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/video/people.mp4'\n",
    "        \n",
    "    run_object_detection(source=VIDEO_SOURCE, flip=False, use_popup=False, model=det_ov_model, device=\"AUTO\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main() or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88b28d-148b-4285-ac4b-f51d9783d3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
